# [ 모델 평가 ]

이유 : 지도학습은 예측력을 높이기 위해, 비지도학습은 잘 grouping 하여 cluster를 생성하기 위해( 추천시스템을 만들려고)

## 1. train_test_split으로 데이터 분리해 학습

train을 전부 학습시키면 test를 예측하기가 어렵기 때문에

-> train_test_split을 사용한다

-> 단점 : 데이터를 학습한지 오래될 수록 학습능력이 떨어지기 때문에 반복적으로 학습해줄 필요가있다.



## 2. 교차 검증 모델

여러번 반복해서 사용한다

- K-fold 교차검증 :  cross_val_score 

- 여러번 반복 수행 :  RepeatedK-Fold

  

## 3. 회귀분석(지도학습, target label)

회귀분석의 예측값을 target label과 비교해서 모델평가 : 평균 제곱 오차



## 4. 분류분석(지도학습, target label) : TP, TN, FP, FN

TP, TN, FP, FN의 confusion_matrix(), 정확도, 정밀도, 재현율 값을 비교 평가한다

 True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)
 False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)
 False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)
 True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)



## 5. 군집평가

클러스터 내의 샘플 데이터간의 거리는 가깝고, 클러스터간의 거리는 멀수록 좋다

이 거리 값들(silhouette_score, 실루엣 계수)을 평가한다



## 6. ROC : logistic regression 임계값 평가 

logistic regression 로 이진분류 수행시 임계값 평가 가능

TPR과 FPR을 비교해주는 ROC 곡선을 사용, 

predict_proba()로 예측 확률을 계산

곡선이 위로 올라가고 곡선 아래 면적이 커질수록 좋은 모델로 평가할 수 있음

: roc_auc_score() 함수의 곡선 아래 면적이 1에 가까울수록 좋은 모델이다



## 학습곡선

train data 량이 학습 알고리즘 성능에 영향을 주는지 검증 => 학습곡선(정확도와 재현율 교차검증)

: learning_curve()



## 보고서 생성

분류 모델의 평가 결과를 보고서로 생성 : classification_report()



## 모델 성능 영향 평가를 위한 시각화

학습 알고리즘의 모델 성능에 영향을 주는 파라미터( 하이퍼파라미터 )의 값 범위를 사용해서 모델 성능 영향을 평가하기 위해 시각화 : validation_curve



# [ 모델 비교 평가 ]

## GridSearchCV 

: 최선의 모델을 선택하기 위해 사용하는 완전 탐색 방법 수행

+ 교차검증, 정확도, 재현율, 정밀도 모두 확인 
+ 비용이 많이든다

## RandomizedSearchCV

: 최선의 모델을 선택하기 위해 비용을 줄여주는 랜덤 서치를 사용하는 탐색 방법 수행

