# [ 특성 선택을 사용한 차원 축소 ]

## 1. 특성선택(feature selection) : 고품질의 정보가 많은 특성은 선택, 덜 유용하면 버리는 방식

### 1 ) 필터 : 통계적인 속성을 조사하여 가장 뛰어난 특성 선택

### 2 ) 래퍼(wrapper) : 시행착오를 통해 가장 높은 품질의 예측을 만드는 특성의 부분조합 선택

### 3 ) 임베디드(embedded) : 학습 알고리즘의 훈련 단계를 확장하거나 일부로 구성하여, 가장 좋은 특성의 부분 조합을 선택



## 2. 어떤 값을 기준으로 특성을 선택할 것인가?

### 1 ) 분산을 기준으로 수치 특성 선택

: 분산이 높은 특성이 분산이 낮은 특성보다 효과적이거나 유용할 것이다

: 분산 기준 설정 VT(Variance Threshold)는 특성의 제곱단위이므로 특성의 단위가 서로 다르면 VT가 동작하지 않는다

: 특성이 평균이 0 이고 단위 분산으로 표준화되어있으면  분산 기준 선택방식이 올바르게 작동하지 않는다

### 2 ) 분산을 기준으로 이진 특성 선택

: 이진 범주형 특성에서 베르누이 확률 변수의 분산이 기준값 이상인 특성을 선택할 수 있도록 분산이 낮은 특성을 삭제한다

: p는 클래스 1의 샘플 비율

: p값을 설정하여 샘플의 대다수가 한 개의 클래스에 속한 특성을 삭제할 수 있다

### 3 ) 상관관계가 큰 특성 삭제

: 가장 많이 사용하는 방법 중 하나

: 상관관계가 너무 크다면, 담고 있는 정보가 매우 비슷하므로 중복된 특성을 포함하는 것이라고 볼 수 있다

: 특성 행렬에서 상관관계 행렬을 사용하여, 상관관계가 큰 특성을 확인하고, 이들 중 하나를 삭제한다

* 상관관계 행렬 생성 -> 크게 상관된 특성의 쌍 확인 -> 그 중 하나를 삭제

### 4 ) 분류 작업에 관련 없는 특성 삭제

: 관련 없는 특성 삭제를 위해 각 특성과 타깃 벡터 사이의 카이제곱 통계를 계산한다

: 카이제곱 통계는 두 범주형 벡터의 독립성을 평가한다

: 특성과 타깃 벡터 사이의 카이제곱 통계를 계산하면 둘 사이의 독립성을 측정할 수 있다

: 사이킷런에서는 SelectBest를 사용하여 통곗값이 가장 좋은 특성을 선택할 수 있습니다.

: 수치형 특성이 있다면 수치형을 범주형 특성으로 변환하여 카이제곱 특성을 사용할 수 있습니다.

: 카이제곱 방식을 사용하려면 모든 값이 음수가 아니어야 합니다.

: 특성이 수치형 특성이라면 f_classif 사용하여 각 특성과 타깃 벡터 사이에 분산 분석(ANOVA)와 F-값 통계를 계산 할 수 있습니다.

: 예] 이진 타깃 벡터인 성별과 수치형 특성인 시험 점수가 있다면, F-값 점수는 남성의 평균 테스트 점수가 여성의 평균 테스트 점수보다 다른지를 설명합니다.

### 5 ) 재귀적 특성제거(recursive feature elimination)

: 모델 성능이 나빠질 때까지 특성을 제거하면서 반복적으로 모델을 훈련하면 자동으로 최선의 특성만 남게 된다

: 재귀적 특성 제거를 교차검증으로 수행할 수 있다















