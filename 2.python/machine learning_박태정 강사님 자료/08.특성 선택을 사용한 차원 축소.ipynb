{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 특성 선택을 사용한 차원 축소\n",
    "## 1 ) 분산을 기준으로 수치 특성 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "iris = datasets.load_iris() #데이터를 로드\n",
    "features = iris.data # 특성과 타깃을 만듭니다\n",
    "target = iris.target\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=.5) # 기준값을 만듭니다.\n",
    "features_high_variance = thresholder.fit_transform(features) # 기준값보다 높은 특성을 선택합니다.\n",
    "features_high_variance[0:3] # 선택한 특성을 확인\n",
    "thresholder.variances_ # 분산 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 특성이 평균이 0이고 단위 분산으로 표준화 되어 있으면 분산 기준 선택 방식은 올바르게 동작하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() # 특성 행렬을 표준화합니다.\n",
    "features_std = scaler.fit_transform(features)\n",
    "selector = VarianceThreshold() # 각 특성의 분산을 계산합니다.\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ) 분산을 기준으로 이진 특성 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16, 0.16, 0.24])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "features = [[0, 1, 0], # 예제 특성 행렬\n",
    "[0, 1, 1], # 특성 0: 80%가 클래스 0\n",
    "[0, 1, 0], # 특성 1: 80%가 클래스 1\n",
    "[0, 1, 1], # 특성 2: 60%가 클래스 0, 40%는 클래스 1\n",
    "[1, 0, 0]]\n",
    "# 분산을 기준으로 선택합니다.\n",
    "\n",
    "#  베르누이 확률 변수의 분산 계산을 위해 threshold를 다음과 같이 설정\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75))) \n",
    "thresholder.fit_transform(features)\n",
    "thresholder.variances_\n",
    "\n",
    "import numpy as np\n",
    "np.var(features, axis=0) #넘파이 var 함수를 사용하여 분산을 계산합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ) 상관관계가 큰 특성 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 상관관계가 큰 두 개의 특성을 가진 특성 행렬을 만듭니다.\n",
    "features = np.array([[1, 1, 1], \n",
    "                     [2, 2, 0], \n",
    "                     [3, 3, 1], \n",
    "                     [4, 4, 0], \n",
    "                     [5, 5, 1], \n",
    "                     [6, 6, 0], \n",
    "                     [7, 7, 1], \n",
    "                     [8, 7, 0], \n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# 특성 행렬을 DataFrame으로 변환\n",
    "dataframe = pd.DataFrame(features) \n",
    "# 상관관계 행렬을 만듭니다.\n",
    "corr_matrix = dataframe.corr().abs() \n",
    "\n",
    "# 상관관계 행렬의 상삼각(upper triangle) 행렬을 선택합니다.\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# 상관 계수가 0.95보다 큰 특성 열의 인덱스를 찾습니다.\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# 특성을 삭제합니다.\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3) \n",
    "#상관관계 행렬\n",
    "dataframe.corr() \n",
    "#상관관계 행렬의 상삼각 행렬\n",
    "upper \n",
    "\n",
    "#상관관계 행렬은 넘파이 corrcoef()로 구할 수 있습니다.\n",
    "#corrcoef()는 특성이 행에 놓여 있을 것으로 가정합니다.\n",
    "#특성이 열에 놓여 있다고 알려주려면 rowvar 매개변수를 False로 지정합니다.\n",
    "np.corrcoef(features, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.triu()는 주어진 배열에서 상삼각 행렬을 추출하여 반환합니다.\n",
    "# 매개변수 k가 기본값 0이면 반환되는 행렬에 대각원소가 포함됩니다.\n",
    "# k값이 커질수록 대각원소에서 k만큼 떨어진 삼각행렬을 반환합니다.\n",
    "# 예) k=2일 경우 주대각선에서 2만큼 떨어진 원소부터 포함됩니다.\n",
    "np.triu(np.ones((4, 4)), k=2)\n",
    "# np.tril()는 주어진 배열에서 하삼각 행렬을 추출 반환합니다.\n",
    "np.tril(np.ones((4, 4)), k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ) 분류 작업에 관계 없는 특성 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수: 4\n",
      "줄어든 특성 개수: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "# 데이터 로드\n",
    "iris = load_iris() \n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 범주형 데이터를 정수형으로 변환\n",
    "features = features.astype(int) \n",
    "\n",
    "# 카이제곱 통계값이 가장 큰 특성 두 개를 선택\n",
    "chi2_selector = SelectKBest(chi2, k=2) \n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"원본 특성 개수:\", features.shape[1]) \n",
    "print(\"줄어든 특성 개수:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수: 4\n",
      "줄어든 특성 개수: 2\n"
     ]
    }
   ],
   "source": [
    "# F-값이 가장 높은 특성 두 개를 선택합니다.\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "print(\"원본 특성 개수:\", features.shape[1]) # 결과 확인\n",
    "print(\"줄어든 특성 개수:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수: 4\n",
      "줄어든 특성 개수: 3\n"
     ]
    }
   ],
   "source": [
    "# 특정 특성 개수를 선택하는 대신 Selectpercentile를 사용하여 특성의 상위 n 퍼센트를 선택할 수 있습니다.\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "# 가장 큰 F-값의 상위 75% 특성을 선택합니다.\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "print(\"원본 특성 개수:\", features.shape[1]) # 결과 선택\n",
    "print(\"줄어든 특성 개수:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 카이제곱 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.28712871,   5.02267003, 133.06854839,  74.27906977])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target\n",
    "#특성 행렬의 차원을 (3, 50, 4)로 바꾸어 클래스별 합을 구합니다.\n",
    "observed = np.sum(features.reshape(3, 50, 4), axis=1)\n",
    "observed\n",
    "#특성 타깃과 전혀 관계없다면 기대 빈도는 전체 합을 클래스 개수 3으로 나눈 값이 됩니다.\n",
    "expected = features.sum(axis=0) / 3\n",
    "expected\n",
    "#카이제곱 공식에 위헤서 구한 observed와 expected를 대입합니다.\n",
    "np.sum((observed - expected)**2 / expected, axis=0)\n",
    "#카이제곱 값이 큰 세 번째, 네 번째 특성이 선택됩니다. chi2_selector객체의 scores_속성에 저장\n",
    "chi2_selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANOVA 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  81.19715 ,   33.715004, 1160.0116  ,  385.483   ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANOVA 직접 계산\n",
    "##전체 평균과 클래스 평균을 계산\n",
    "total_mean = np.mean(features, axis=0)\n",
    "total_mean\n",
    "class_mean = np.mean(features.reshape(3, 50, 4), axis=1)\n",
    "class_mean\n",
    "#ss_total 계산\n",
    "ss_between = np.sum(50 * (class_mean - total_mean)**2, axis=0)\n",
    "ss_between\n",
    "ss_total = np.sum((features - total_mean)**2, axis=0)\n",
    "ss_total\n",
    "#ss_beteen과 ss_tatal을 F-값 공식에 대입\n",
    "f = (ss_between/(3-1)) / ((ss_total-ss_between)/(150-3))\n",
    "f\n",
    "fvalue_selector.scores_ #F-값 scores_속성에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 ) 재귀적 특성 제거 ( recursive feature elimination )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  0.7031277 , -0.925066  ],\n",
       "       [-1.07500204,  2.56148527,  0.4746258 ],\n",
       "       [ 1.37940721, -1.77039484, -0.39616889],\n",
       "       ...,\n",
       "       [-0.80331656, -1.60648007,  0.25068305],\n",
       "       [ 0.39508844, -1.34564911, -1.35054293],\n",
       "       [-0.55383035,  0.82880112,  0.14050409]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# 특성 행렬과 타깃 벡터를 생성합니다.\n",
    "# make_regression : 회귀분석을 수행하는데 필요한 모의 데이터를 생성해준다\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "n_features = 100, # 특성 개수\n",
    "n_informative = 2, # 실제 회귀에 연관성이 높은 특성 개수 설정 \n",
    "random_state = 1)\n",
    "\n",
    "# 선형 회귀 모델을 만듭니다.\n",
    "ols = linear_model.LinearRegression()\n",
    "# 재귀적으로 특성을 제거합니다.\n",
    "# neg_mean_squared_error : 평균 제곱 오차 사용\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\") # 객체 생성\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.n_features_ # 최선의 특성 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.support_ # 선택된 특성이 표시된 불리언 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 15, 94, 45, 13,  1, 35,  6, 31, 81, 36, 25,  2,  7, 53, 40, 27,\n",
       "       46, 75, 65, 61, 68, 92,  9, 39, 48, 98, 96, 47, 71, 19, 37, 11, 20,\n",
       "       50,  4, 33, 42, 67,  1, 43, 63, 85, 86, 56, 60,  5, 16,  8, 55, 93,\n",
       "       73, 10, 76,  1, 77, 52, 24, 58, 62, 21, 82, 72, 90, 80, 91, 18, 30,\n",
       "       57, 89, 64, 51, 59, 17, 28, 32, 49, 66, 87, 84, 38, 88, 34, 44, 14,\n",
       "       79, 41, 12, 29, 23,  3, 78, 22, 95, 26, 70, 54, 83, 74, 97])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.ranking_ # 특성의 순위: 최고(1)에서 최악(96)까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=ols, n_features_to_select=3)\n",
    "rfe.fit(features, target)\n",
    "rfe.transform(features)\n",
    "# rfe객체가 선택한 특성이 rfecv 객체가 선택한 특성과 동일한지 확인하기 위해 불리언 마스크를 비교\n",
    "np.all(rfe.support_ == rfecv.support_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
